<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Talk to Bird's Eye View Objects in the Scene.">
  <meta name="keywords" content="LLM, BEV, Talk2BEV">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Talk2BEV: Language-Enhanced BEV Maps</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icons8-chat-96.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://robotics.iiit.ac.in">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://conceptfusion.github.io">
            ConceptFusion
          </a>
          <a class="navbar-item" href="https://vikr-182.github.io/UAP-BEV/">
            UAP-BEV
          </a>          
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-3 publication-title">Talk2BEV: Language-Enhanced Bird's Eye View (BEV) Maps</h1>
          <h2 class="title is-6 publlication-title">Preprint</h2>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://vikr-182.github.io/">Vikrant Dewangan</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://tusharc31.github.io/">Tushar Choudhary</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=ZER2BeIAAAAJ&hl=en">Shivam Chandhok</a><sup>2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://rudeninja.github.io/">Shubham Priyadarshan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://anushkaj1.github.io">Anushka Jain</a><sup>1</sup>,
            </span>
            </div>

            <div class="is-size-6 publication-authors">            
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=0zgDoIEAAAAJ&hl=en">Arun K. Singh</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://siddharthsrivastava.github.io/">Siddharth Srivastava</a><sup>4</sup>
            </span>
            <span class="author-block">
              <a href="https://krrish94.github.io/">Krishna Murthy Jatavallabhula</a><sup>5</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=QDuPGHwAAAAJ&hl=en">K. Madhava Krishna</a><sup>1</sup>
            </span>                        
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>IIIT Hyderabad</span>
            <span class="author-block"><sup>2</sup>University of British Columbia</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>3</sup>University of Tartu</span>
            <span class="author-block"><sup>4</sup>TensorTour Inc</span>
            <span class="author-block"><sup>5</sup>MIT</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="assets/pdf/talk2bev.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=TMht-8SGJ0I"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/llmbev/talk2bev-code"
                   class="external-link button is-normal is-rounded is-dark" disabled="true">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/llmbev/"
                   class="external-link button is-normal is-rounded is-dark" disabled="true">
                  <span class="icon">
                      <i class="fab fa-database"></i>
                  </span>
                  <span>Bench</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
    <div  class="container is-max-desktop">
      <img src="./static/images/talk2bev_teaser-1.png" class="interpolation-image" alt="Interpolate start reference image."/>
      <div class="is-size-7">
        We build <i>Language-enhanced Bird's-Eye View (BEV) maps</i> using (a) BEV representations constructed from vehicle sensors (multi-view images, lidar), and (b) Aligned vision-language features for each object which can be directly used as context within large vision-language models (LVLMs) to query and <i>talk</i> to the objects in the scene. These maps embed knowledge about object semantics, material properties, affordances, and spatial concepts and can be queried for visual reasoning, spatial understanding, and making decisions about potential future scenarios, critical for autonomous driving application.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="">
        <div class="item">
          <video poster="" id="laundry-bag" autoplay controls muted loop playsinline height="100%" width="100%">
            <source src="./static/videos/overtaking.mp4" type="video/mp4">
          </video>
        </div>      
      <!-- <div class="carousel results-carousel">
        <div class="item item-steve">

        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/parking.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce Talk2BEV, a large vision-
            language model (LVLM) interface for bird’s-eye view (BEV)
            maps commonly used in autonomous driving. 
          </p>
          <p>
            While existing
            perception systems for autonomous driving scenarios have
            largely focused on a pre-defined (closed) set of object categories
            and driving scenarios, Talk2BEV eliminates the need for BEV-
            specific training, relying instead on performant pre-trained
            LVLMs. This enables a single system to cater to a variety
            of autonomous driving tasks encompassing visual and spatial
            reasoning, predicting the intents of traffic actors, and decision-
            making based on visual cues. 
          </p>
          <p>
            We extensively evaluate Talk2BEV
            on a large number of scene understanding tasks that rely
            on both the ability to interpret freefrom natural language
            queries, and in grounding these queries to the visual context
            embedded into the language-enhanced BEV map. To enable
            further research in LVLMs for autonomous driving scenarios,
            we develop and release Talk2BEV-Bench, a benchmark encom-
            passing 1000 human-annotated BEV scenarios, with more than
            20,000 questions and ground-truth responses from the NuScenes
            dataset.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- i find this not useful 
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/TMht-8SGJ0I"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Approach. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Approach</h2>
        <!-- Interpolating. -->
        <div class="content has-text-justified">
          <ol>
            <li>
              We input the perspective images \( \mathcal{I} \) and Lidar data \( \mathcal{X} \). 
            </li>
            <li>
              Using a BEV Network, we generate Bird's Eye View (BEV) predictions \( \mathcal{O} \)
            </li>
            <li>
              For each object \( i \) in the BEV Prediction \( \mathcal{O} \), we extract its crop proposals \( r_i \) from the LiDAR-Camera re-projection pipeline, and obtain its captions using Large Vision-Language Models (LVLMs). 
              Each object \( i \) contains in the map \( \mathbf{L} \) its bev information including geometric cues like - area, centroid, and object descriptions like crop captions. 
  . We perform BEV-Image Projection for the objects \( o_i \in \mathcal{O} \).
            </li>
            <li>
              We construct the Language-Enhanced map \( \mathbf{L}(\mathcal{O}) \) by augmenting the generated BEV with aligned image-language features for each object from large vision-language models (LVLMs). These features can directly be used as context to LVLMs for answering object and scene-specific queries. The comprehensive Language Enhanced Map representation encodes objects in the scene along with their semantic descriptions and geometric cues 
            from BEV.
            </li>
          </ol>
        </div>  
        <div class="columns is-vcentered">
          <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/methodology.mp4"
                    type="video/mp4">
          </video>
        </div>      
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!--
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        -->
        <!--/ Re-rendering. -->

      </div>
    </div>

    <h2 class="title is-3 has-text-centered">Spatial Operators</h2>
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h4 class="is-3 has-text-centered">Find distance between ...</h4>
          <!-- <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p> -->
          <img src="./static/images/spatial1.png" class="interpolation-image" alt="Interpolate start reference image."/>
        </div>
      </div>
      <!--/ Visual Effects. -->
      <!-- Matting. -->
      <div class="column">
        <div class="content">
          <h4 class="is-3 has-text-centered">Nearest 2 vehicles in front..</h4>
        </div>
        <div class="columns is-centered">
          <div class="column content">
            <!-- <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p> -->
            <img src="./static/images/spatial2.png" class="interpolation-image" alt="Interpolate start reference image."/>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There are many previous works which bind Language and Vision for Autonomous Driving - <a href="https://arxiv.org/abs/1909.10838">Talk2Car</a>, <a href="https://arxiv.org/abs/2303.03366">ReferKITTI</a>.
          </p>
          <p>
            Concurrent to our work is <a href="https://arxiv.org/abs/2305.14836">NuScenesQA</a> which attempts VQA in autonomous driving scenarios on top of BEV networks.
          </p>
          <p>
            Works like <a href="https://arxiv.org/abs/2309.04379v1">NuPrompt</a> attempt Language-Image grounding with Large-Language models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{talk2bev,
      title = {Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving},
      author = {Dewangan, Vikrant and Choudhary, Tushar and Chandhok, Shivam and Priyadarshan, Shubham and Jain, Anushka and Singh, Arun and Srivastava, Siddharth and Jatavallabhula, {Krishna Murthy} and Krishna, Madhava},
      year = {2023},
      booktitle = {arXiv},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
